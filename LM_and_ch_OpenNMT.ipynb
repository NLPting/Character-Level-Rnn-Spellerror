{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "\n",
    "API_URL = \"http://api.netspeak.org/netspeak3/search?query=%s\"\n",
    "\n",
    "class NetSpeak:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "        self.page = None\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __getPageContent(self, url):\n",
    "        return requests.get(url, headers=self.headers).text\n",
    "        # return self.opener.open(url).read()\n",
    "\n",
    "    def __rolling(self, url, maxfreq=None):\n",
    "        if maxfreq:\n",
    "            webdata = self.__getPageContent(url + \"&maxfreq=%s\" % maxfreq)\n",
    "        else:\n",
    "            webdata = self.__getPageContent(url)\n",
    "        if webdata:\n",
    "            # webdata = webdata.decode('utf-8')\n",
    "            results = [data.split('\\t') for data in webdata.splitlines()]\n",
    "            results = [(data[2], float(data[1])) for data in results]\n",
    "            lastFreq = int(results[-1][1])\n",
    "            if lastFreq != maxfreq:\n",
    "                return results + self.__rolling(url, lastFreq)\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def search(self, query):\n",
    "        if query in self.dictionary: return self.dictionary[query]\n",
    "        \n",
    "        queries = query.lower().split()\n",
    "        new_query = []\n",
    "        for token in queries:\n",
    "            if token.count('|') > 0:\n",
    "                new_query.append('[+{0}+]'.format('+'.join(token.split('|'))))\n",
    "            elif token == '*':\n",
    "                new_query.append('?')\n",
    "            else:\n",
    "                new_query.append(token)\n",
    "        new_query = '+'.join(new_query)\n",
    "        url = API_URL % (new_query.replace(' ', '+'))\n",
    "        self.dictionary[query] = self.__rolling(url)\n",
    "        return self.dictionary[query]\n",
    "    \n",
    "SE = NetSpeak() # singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, re\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "count = dict()\n",
    "count_c = defaultdict(lambda: 0)\n",
    "for line in open('count_1edit.txt', 'r', encoding='utf8'):\n",
    "    wc, num = line.strip().split('\\t')\n",
    "    w, c = wc.split('|')\n",
    "    count[(w, c)] = int(num)\n",
    "    count_c[c] += int(num)\n",
    "Ncount = Counter(count.values())\n",
    "\n",
    "Nall = len(count.keys())\n",
    "N0 = 26*26*26*26+2*26*26*26+26*26 - Nall\n",
    "Nr = [ N0 if r == 0 else Ncount[r] for r in range(12) ]\n",
    "\n",
    "def smooth(count, r=10):\n",
    "    if count <= r:\n",
    "        return (count+1)*Nr[count+1] / Nr[count]\n",
    "    else:\n",
    "        return count\n",
    "\n",
    "def Pedit(w, c):\n",
    "    if (w, c) not in count and count_c[c] > 0:\n",
    "        return smooth(0) / count_c[c]\n",
    "    if count_c[c] > 0:\n",
    "        return smooth(count[(w, c)]) / count_c[c]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "# WORDS = Counter(open('big.txt').read().split())\n",
    "\n",
    "def Pw(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    states = [ ('', word, 0, Pw(word), 1) ]\n",
    "    for i in range(len(word)):\n",
    "        # print(i, states[:3])\n",
    "        STATES = [ s for state in states for s in next_states(state) ]\n",
    "        states = sorted(STATES, key=lambda x: x[2])\n",
    "\n",
    "        unique, new_states = set(), []\n",
    "        for state in states:\n",
    "            if state[0] + state[1] in unique: continue\n",
    "\n",
    "            unique.add(state[0] + state[1])\n",
    "            new_states.append(state)\n",
    "        states = new_states\n",
    "        states = sorted(states, key=lambda x: P(x[3], x[4]), reverse=True) [:500]# [:MAXBEAM]\n",
    "    return states[:10]\n",
    "\n",
    "def next_states(state):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    L, R, edit, prob, ped = state\n",
    "    R0, R1 = R[0], R[1:]\n",
    "    if edit == 2: return [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    noedit    = [( L + R0, R1, edit, prob, ped*0.8 )]\n",
    "    delete    = [( L, R1, edit+1, Pw(L + R1), ped * Pedit(L[-1]+R0, L[-1]))]  if len(L) > 0 else []\n",
    "    insert    = [( L + R0 + c, R1, edit+1, Pw(L + R0 + c + R1), ped * Pedit(R0, R0 + c) ) for c in letters]\n",
    "    replace   = [( L + c, R1, edit+1, Pw(L + c + R1), ped * Pedit(R0, c) ) for c in letters]\n",
    "    transpose = [( L[:-1] + R0 + L[-1], R1, edit+1, Pw(L[:-1] + R0 + L[-1] + R1), ped * Pedit(L[-1]+R0, R0+L[-1]) )] if len(L) > 1 else []\n",
    "    return set(noedit + delete + replace + insert + transpose)\n",
    "\n",
    "'''Combining channel probability with word probability to score states'''\n",
    "def P(pw, pedit):\n",
    "    return pw*pedit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_word = open('lab4.confusables.txt','r').readlines()\n",
    "Confuse = {}\n",
    "for line in confuse_word:\n",
    "    w ,c = line.split('\\t')\n",
    "    Confuse[w]=c.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(tokens):\n",
    "    return [tokens[i:i+3] for i in range(len(tokens) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_where(tm):\n",
    "    trigrams = get_trigrams(tm)\n",
    "    tri_tmp = []\n",
    "    for index,tri in enumerate(trigrams):\n",
    "        #print(tri)\n",
    "        res = SE.search(' '.join(tri))\n",
    "        #print(res)\n",
    "        if res:\n",
    "            tri_tmp.append((index,res[0][1],tri))\n",
    "        else:\n",
    "            tri_tmp.append((index,0,tri))\n",
    "    #print(tri_tmp)\n",
    "    minn  = min(tri_tmp,key=lambda x:x[1])[2]\n",
    "    #print(minn)\n",
    "    for find_index in tri_tmp:\n",
    "        #print(find_index[2])\n",
    "        if find_index[2]==minn:\n",
    "            detect_sentence = find_index\n",
    "            \n",
    "    return detect_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters.\n"
     ]
    }
   ],
   "source": [
    "import onmt\n",
    "import onmt.io\n",
    "import onmt.translate\n",
    "import onmt.ModelConstructor\n",
    "import io\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "# Load the model.\n",
    "Opt = namedtuple('Opt', ['model', 'data_type', 'reuse_copy_attn', \"gpu\"])\n",
    "opt = Opt(\"ch-merge-model/demo_model_acc_91.31_ppl_1.70_e13.pt\", \"text\",False, 0)\n",
    "fields, model, model_opt =  onmt.ModelConstructor.load_test_model(opt,{\"reuse_copy_attn\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_OpenNMT_generate_candidate(detect_sentence_arr):\n",
    "    ch_candidate = {}\n",
    "    \n",
    "    text = '\\n'.join(' '.join(word) for word in detect_sentence_arr)\n",
    "    input_text = io.StringIO(text)\n",
    "    \n",
    "    data = onmt.io.build_dataset(fields, \"text\", input_text, None, use_filter_pred=False)\n",
    "    data_iter = onmt.io.OrderedIterator(\n",
    "        dataset=data, device=0,\n",
    "        batch_size=1, train=False, sort=False,\n",
    "        sort_within_batch=True, shuffle=False)\n",
    "    # Translator\n",
    "    scorer = onmt.translate.GNMTGlobalScorer(None,\n",
    "                                             None,\n",
    "                                             None,\n",
    "                                             None)\n",
    "    # Translator\n",
    "    translator = onmt.translate.Translator(model, fields,\n",
    "                                               beam_size=10,\n",
    "                                               n_best=5,\n",
    "                                               global_scorer=scorer,\n",
    "                                               cuda=True)\n",
    "    builder = onmt.translate.TranslationBuilder(\n",
    "            data, translator.fields,\n",
    "            5, False, None)\n",
    "    # Translator\n",
    "    scorer = onmt.translate.GNMTGlobalScorer(None,\n",
    "                                             None,\n",
    "                                             None,\n",
    "                                             None)\n",
    "    \n",
    "    translator = onmt.translate.Translator(model, fields,\n",
    "                                               beam_size=20,\n",
    "                                               n_best=5,\n",
    "                                               global_scorer=scorer,\n",
    "                                               cuda=True)\n",
    "    builder = onmt.translate.TranslationBuilder(\n",
    "            data, translator.fields,\n",
    "            5, False, None)\n",
    "    for batch in data_iter:\n",
    "        batch_data = translator.translate_batch(batch, data)\n",
    "        translations = builder.from_batch(batch_data)\n",
    "        for trans in translations:\n",
    "            n_best_preds = [\" \".join(pred) for pred in trans.pred_sents[:5]]\n",
    "        \n",
    "        ch_candidate[' '.join(translations[0].src_raw).replace(' ','')] = n_best_preds\n",
    "    \n",
    "    \n",
    "    return ch_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average src size 5.666666666666667 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/ting/Character-Level-Rnn-Spellerror/onmt/modules/GlobalAttention.py:176: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  align_vectors = self.sm(align.view(batch*targetL, sourceL))\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:40: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gost': ['ghost', 'got', 'goes', 'most', 'go'],\n",
       " 'noicey': ['noisy', 'noise', 'noisy,', 'monkeys', 'noise.'],\n",
       " 'weanter': ['weather', 'winter', 'water', 'weather,', 'whether']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_OpenNMT_generate_candidate(['noicey', 'gost', 'weanter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find_the_best(tm,start):\n",
    "    best = (None, None, None, None, -math.inf)\n",
    "    #find_the_best = []\n",
    "    for i in range(start,start+3):\n",
    "        candidate = []\n",
    "        candidate = ch_candidate[word[i]]\n",
    "        if tm[i] in Confuse.keys():\n",
    "            candidate.append(tm[i])\n",
    "        #print(candidate)\n",
    "        for cancan in candidate:\n",
    "            count = 1.0\n",
    "            combine = tm[:i] + [cancan] + tm[i+1:]\n",
    "            #print(combine)\n",
    "            trigrams = get_trigrams(combine)\n",
    "            \n",
    "            for tri in trigrams:\n",
    "                res = SE.search(' '.join(tri))\n",
    "                count *= res[0][1] if res else 0\n",
    "                #print(res,count)\n",
    "                \n",
    "            best = (combine,tm[i],cancan,candidate,count) if count > best[-1] else best\n",
    "       \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分割正確跟錯誤的資料集\n",
    "line = open('lab4.test.1.txt','r').readlines()\n",
    "Correct_sentence = []\n",
    "False_sentence = []\n",
    "for sentence in line:\n",
    "    tmp = sentence.split('\\t')\n",
    "    False_sentence.append(tmp[0].strip().lower())\n",
    "    Correct_sentence.append(tmp[1].strip().lower())\n",
    "test_Correct=Correct_sentence[:20]\n",
    "test_False = False_sentence[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_where(tm):\n",
    "    trigrams = get_trigrams(tm)\n",
    "    tri_tmp = []\n",
    "    for index,tri in enumerate(trigrams):\n",
    "        #print(tri)\n",
    "        res = SE.search(' '.join(tri))\n",
    "        #print(res)\n",
    "        if res:\n",
    "            tri_tmp.append((index,res[0][1],tri))\n",
    "        else:\n",
    "            tri_tmp.append((index,0,tri))\n",
    "    \n",
    "    minn  = min(tri_tmp,key=lambda x:x[1])[2]\n",
    "    for find_index in tri_tmp:\n",
    "        #print(find_index[2])\n",
    "        if find_index[2]==minn:\n",
    "            detect_sentence = find_index\n",
    "            \n",
    "    return detect_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average src size 4.666666666666667 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/ting/ch-OpenNMT-py/onmt/modules/GlobalAttention.py:176: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  align_vectors = self.sm(align.view(batch*targetL, sourceL))\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:40: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:strang\n",
      "Candidates: ['strange', 'strong', 'straight', 'strong,', 'starting']\n",
      "Correction: strange\n",
      "i felt very strang -> i felt very strange\n",
      "hits = 1\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:brake\n",
      "Candidates: ['break', 'breaks', 'breaking', 'broken', 'broke', 'brake']\n",
      "Correction: break\n",
      "at brake time -> at break time\n",
      "hits = 2\n",
      "\n",
      "average src size 4.0 3\n",
      "Error:brack\n",
      "Candidates: ['black', 'break', 'barack', 'back', 'breach']\n",
      "Correction: back\n",
      "when the brack was finished -> when the back was finished\n",
      "hits = 2\n",
      "\n",
      "average src size 4.0 3\n",
      "Error:weanter\n",
      "Candidates: ['weather', 'winter', 'water', 'weather,', 'whether']\n",
      "Correction: water\n",
      "in the weanter when it was snowing -> in the water when it was snowing\n",
      "hits = 2\n",
      "\n",
      "average src size 2.6666666666666665 3\n",
      "Error:gost\n",
      "Candidates: ['ghost', 'got', 'goes', 'most', 'go']\n",
      "Correction: most\n",
      "i thought it was a gost -> i thought it was a most\n",
      "hits = 2\n",
      "\n",
      "average src size 5.0 3\n",
      "Error:expect\n",
      "Candidates: ['except', 'expect', 'expected', 'expert', 'ubject']\n",
      "Correction: except\n",
      "everything expect the houses -> everything except the houses\n",
      "hits = 3\n",
      "\n",
      "average src size 4.0 3\n",
      "Error:steped\n",
      "Candidates: ['stepped', 'stabbed', 'stopped', 'swept', 'stripped']\n",
      "Correction: stepped\n",
      "when i first steped -> when i first stepped\n",
      "hits = 4\n",
      "\n",
      "average src size 4.333333333333333 3\n",
      "Error:exclation\n",
      "Candidates: ['evaluation', 'education', 'evolution', 'educating', 'evacuation']\n",
      "Correction: evaluation\n",
      "i was on an exclation -> i was on an evaluation\n",
      "hits = 4\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:i\n",
      "Candidates: ['i', \"i'm\", 'in', 'is', 'it']\n",
      "Correction: i\n",
      "i noicey that i was on this thing -> i noicey that i was on this thing\n",
      "hits = 4\n",
      "\n",
      "average src size 5.0 3\n",
      "Error:fance\n",
      "Candidates: ['fence', 'france', 'fancy', 'dance', 'fiancee']\n",
      "Correction: fence\n",
      "through the fance -> through the fence\n",
      "hits = 5\n",
      "\n",
      "average src size 5.0 3\n",
      "Error:kille\n",
      "Candidates: ['killed', 'kill', 'mile', 'like', 'life']\n",
      "Correction: killed\n",
      "the hunters kille them -> the hunters killed them\n",
      "hits = 5\n",
      "\n",
      "average src size 5.0 3\n",
      "Error:nerrow\n",
      "Candidates: ['narrow', 'nervous', 'nero', 'network', 'nephew']\n",
      "Correction: network\n",
      "they kill birds with their nerrow -> they kill birds with their network\n",
      "hits = 5\n",
      "\n",
      "average src size 3.0 3\n",
      "Error:depe\n",
      "Candidates: ['deepen', 'dept', 'deep', 'dpt.', 'epe']\n",
      "Correction: deep\n",
      "make a depe hole -> make a deep hole\n",
      "hits = 6\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:gardon\n",
      "Candidates: ['garden', 'ground', 'garage', 'gardens', 'grand']\n",
      "Correction: garden\n",
      "to tidy up his gardon -> to tidy up his garden\n",
      "hits = 7\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:belu\n",
      "Candidates: ['blue', 'bel', 'belt', 'blew', 'blue,']\n",
      "Correction: blew\n",
      "the wind belu the leaves -> the wind blew the leaves\n",
      "hits = 8\n",
      "\n",
      "average src size 2.3333333333333335 3\n",
      "Error:mr\n",
      "Candidates: ['mr', 'mr.', 'rm', 'ms', 'mrs']\n",
      "Correction: mr\n",
      "mr j. was very angray -> mr j. was very angray\n",
      "hits = 8\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:leavs\n",
      "Candidates: ['leaves', 'lives', 'leave', 'left', 'live']\n",
      "Correction: leaves\n",
      "garden full of leavs -> garden full of leaves\n",
      "hits = 9\n",
      "\n",
      "average src size 3.6666666666666665 3\n",
      "Error:manger\n",
      "Candidates: ['manager', 'manager,', 'manager.', 'manage', 'managers']\n",
      "Correction: manager\n",
      "talk to the manger -> talk to the manager\n",
      "hits = 10\n",
      "\n",
      "average src size 3.3333333333333335 3\n",
      "Error:throw\n",
      "Candidates: ['through', 'throw', 'thrown', 'threw', 'throws']\n",
      "Correction: through\n",
      "they throw a aero -> they through a aero\n",
      "hits = 10\n",
      "\n",
      "average src size 4.666666666666667 3\n",
      "Error:an\n",
      "Candidates: ['and', 'a', 'in', 'am', 'any']\n",
      "Correction: and\n",
      "an ansion method of hunting -> and ansion method of hunting\n",
      "hits = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for i,line in enumerate (test_False):\n",
    "    word = line.split(' ')\n",
    "    detect_sentence = detect_where(word)\n",
    "    start = detect_sentence[0]\n",
    "    \n",
    "    ch_candidate = ch_OpenNMT_generate_candidate(detect_sentence[2])\n",
    "    \n",
    "    combine ,wrong ,right ,candidate ,_ =find_the_best(word,start)\n",
    "    combine = ' '.join(combine).strip()\n",
    "    if combine == test_Correct[i]:\n",
    "        hits+=1\n",
    "        \n",
    "    print(\"Error:\" +  str(wrong))\n",
    "    print(\"Candidates:\", candidate)\n",
    "    print(\"Correction:\", right)\n",
    "    print(test_False[i], \"->\", combine )\n",
    "    print(\"hits =\", hits)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
